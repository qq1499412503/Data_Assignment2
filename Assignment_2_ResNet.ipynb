{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 2 ResNet",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qq1499412503/Data_Assignment2/blob/master/Assignment_2_ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N33ST9xtSynr",
        "colab_type": "text"
      },
      "source": [
        "#Introduction\n",
        "Our group has chosen image classification for the topic of this project, we also chose to use a deep learning method for the technique to build our model. We obtained our dataset from Cifar10, it is a dataset that contains 60,000 colored images. Our goal is to classify these different images and class types by using Residual Networking or ResNet for short, this is a deep neural network that can pass at least 100 layers and with that, surpasses any deep learning technique that preceded it. The problem with other deep convolutional neural networks is that once the depth increases accuracy and performance saturates and degrades rapidly as well, this is due to the problem of vanishing gradients. As compared to the other neural networks, ResNet is able to overcome depth by introducing identity shortcut connection these are connections that skip one to a few layers which create a block, and also have the advantage of not having to add computational complexity and parameters to the model.\n",
        "\n",
        "We chose to build a ResNet model for our image classification problem as we think that this is the strongest and most innovative type of deep learning techniques available today and also to be able to experiment with how this deep learning technique works for ourselves.\n",
        "\n",
        "In this paper, we will be showing how we designed our ResNet model, how we built and tested our model by comparing results, evaluate the outcome, our conclusion based on what we learned from the project, and finally discuss the ethical values of our model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFh-nxIHUHTY",
        "colab_type": "text"
      },
      "source": [
        "#Exploration\n",
        "For our project, we got the ideas and concepts from Keras and the documentation titled ‘Deep Residual Learning for Image Recognition’. From both these resources, we were able to create our model for ResNet and acquire our dataset Cifar10. Our dataset Cifar10 is a famous collection of images from the Canadian Institute for Advanced Research and this dataset is usually used for machine learning, especially when it comes to image classification. It contains 60,000 images, distributed into 10 classes, the 10 different classes in our dataset cover a variety of photos of different types of animals, we as a group, are building a model to distinguish a certain type based on the variety of photos in our dataset. For our group to be able to do this we split the dataset into 50,000 for training and the other 10,000 for validation. The images are in a 32x32 format and we then crop the image into a 224x224 format to satisfy and optimize our ResNet model.\n",
        "\n",
        "Our ResNet model has 16 blocks to create our ResNet32 model. To further optimize our model, we used both SGD and Adam in batches of 32 for 4 of our models and 16 in 1 with a specific learning rate which we think was suitable depending on the situation to create a fair environment.  We ran the same model several times with different epochs to be able to compare results and see our model’s accuracy depending on the number of epochs we input in the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH8ZOTHZUJwD",
        "colab_type": "text"
      },
      "source": [
        "#Methodology\n",
        "To acquire and load our dataset, we imported it directly from the Keras library instead of downloading the zip file and having to unzip it to our local devices. It’s not only the dataset that we imported from Keras, but there were a lot of packages that we had to get as well in order to build our model.\n",
        "\n",
        "We loaded our dataset into our program and then split the total 60,000 samples to 50,000 for training and 10,000 for testing. Also, to show the difference in the classes of the dataset we labeled them in their respective criteria to have a clearer output and instead of having integers as results.\n",
        "\n",
        "We first preprocessed our data by normalizing our x_train and x_test by subtracting the pixel mean to center the input to 0. We did this as to not have different distribution of feature values and have equal structures for all values of the dataset. We also preprocessed our y_train and y_test by normalizing and converting class vectors into binary class matrices for fitting and prediction purposes. Then we also print and see the changes before and after our preprocessing.\n",
        "\n",
        "For creating our ResNet model we first created BN_layers to improve our model’s accuracy and also to speed up our training, this is essential for it is the ‘shortcut connection’ that ResNet is famous for. For each layer, we used regularization in the value of L2 0.0001. We also created [16, 32, 64] different filters and each of them with a length of 3 and stride of [1, 2].\n",
        "\n",
        "In training our model as mentioned earlier we set a specific learning rate based on the trends of loss value. We also ran the model several times. Some with the same batch size of 32, but with different epochs ranging from 50, 100, 150, and 200 and some we set it at 100 or 150 epochs but only with a batch size of 16."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUyfsWQ-Uj45",
        "colab_type": "text"
      },
      "source": [
        "#Evaluation\n",
        "Based on training in different parameters, the model showing different performance, such as there would be a higher performance if reducing the batch size but it will also cause a low training speed, this project will focus on batch size 32, in addition, all parameters such as learning rate, optimizer, number of epochs, regularization and dropout will impact the trends of loss value. In this project, after several tries, a specific learning rate is used in different numbers of epoch intervals with L2 regularization for training the model based on SGD optimizer as the end.\n",
        "\n",
        "For evaluation, the report will produce the f-score for each class. A confusion matrix will also be presented to be able to show the ability of our model to classify an image for each class. \n",
        "\n",
        "Lastly, loss for both training and testing sets will be shown in the evaluation part to see the changes in the performance and the flow of our model.\n",
        "\n",
        "In analyzing the model, different number of epoch will be tried for the value which can reach a high performance in limited computing resources. \n",
        "\n",
        "Figure 1, loss and accuracy in 100 epoch \t\t\tFigure 2, loss and accuracy in 200 epoch\n",
        "\n",
        "By comparing the trends of loss value in different epoch number, figure 1 showing the loss value in epoch number 100 and figure showing the loss value in epoch number 200, both image showing there is an increasing loss value after 75 epochs and there is also a significant increase happens after 100 epoch. Therefore, 100 epochs were used for training the model.\n",
        "\n",
        "optimizers\n",
        "F1-score\n",
        "SGD\n",
        "0.68\n",
        "Adam\n",
        "0.74\n",
        "RMSprop\n",
        "0.71\n",
        "Adagrad\n",
        "0.67\n",
        "Table 3, different optimizers used in learning rate 0.01\n",
        "\n",
        "In addition, four different optimizers were used for training the model. As table 3 shows, Adam getting the highest f-score even SGD was considering as a better optimizer in most reports. Thus, Adam will used for training the model.\n",
        "\n",
        "Figure 4, learning rate in different value\n",
        "According to figure 4, learning rate in 0.01, 0.001, 0.0005, 0.0001 was tried and the trends of loss value, by comparing 4 plot, both learning rates in 0.01 and 0.0001 showing a high loss value and learning rate in 0.01 also have a significant flowing in loss value, by comparing both 0.001 and 0.0005,  learning rate in 0.001 showing the lowest loss value at the end, in this case, learning rate in 0.001 will be used for training the model.\n",
        "\n",
        "Figure 5, confusion matrix in Adam optimizer with learning 0.001\n",
        "Figure 5 shows the confusion matrix plot based on Adam optimizer with learning rate 0.001, all classes are able to classify correctly in the rate greater than 50%, but for some specific class, it is showing a higher error rate than others.\n",
        "\n",
        "\n",
        "Figure 6, classification report in Adam optimizer with learning 0.001\n",
        "Figure 6 showing the classification report of figure 5, as table show, there is only 0.50 F-Score in cat and 0.57 F-Score for cat.\n",
        "\n",
        "Figure 7, confusion matrix Adam optimizer with learning rate 0.001 with regularization\n",
        "Both Dropout and regularization was considered to avoid overfitting, in this project, we were trying to give L2 regularization in value 0.0001 to each basic layer,  figure 7 showing the confusion matrix after give regularization, there is an improvement happens by comparing with figure 5.\n",
        "\n",
        "Figure 8, classification report in Adam optimizer with learning 0.001 and L2 regularization\n",
        "Figure 8 shows the classification report after regularization, there is an improvement happens to specific class such as cat, bird, and dog. The average F-Score reached 81%.\n",
        "\n",
        "Figure 9, the final F-Score showed in 5 test\n",
        "Figure 9 shows the F-Score in 5 times training, by calculating the average f-score is 0.798 which close to 0.80.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOdHinRLpmV-",
        "colab_type": "text"
      },
      "source": [
        "#Conclusion\n",
        "In conclusion, Resnet is a developed model which can work with resolving most image classification problems. The model in this project is well structured but do have some problems such as did not reach a high average f-score, limited performance for specific class and hard to increase efficiency in the training stage. For improving the model, Resnet provided a pre-trained model which can provide a higher start in training different data and also saves time in training a model under deep learning. In addition, fine-tuning is also important, it is a stage under transfer learning and provide a specific method to increase the model, finally, overfitting is always a significant problem when training a model under deep learning, working with regularization and drop-out will also helps in increasing its performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VTlUmopptQ_",
        "colab_type": "text"
      },
      "source": [
        "#Ethical\n",
        "Our topic of image classification is similar to both object detection and facial recognition, these are two methods more commonly used in today’s world. Due to it being similar to these two other methods, we thought that we can explain much clearer both the benefits and ethical risks by giving more real-world examples and topics.\n",
        "Facial recognition was created around the 1960s and ever since then this technique has been used widely in different fields and has affected people’s daily lives. Even if facial recognition has a lot of benefits and convenience in society, it also has some potential ethical risks, depending on how it is used.\n",
        " \n",
        "Ten years ago, there were more than 50 million cameras used all over the world and in every main city in America, it is evident that every corner is installed with surveillance cameras. Because of this, it has been a very efficient way to find criminal offenders and a lot of these offenders have been arrested and also a lot of criminal cases have been solved. But at the same time, this type of technology can be abused. The first thing we have to consider is accuracy, humans have different skin colors, might have similar facial expressions. For example, the facial recognition system used in America has problems recognizing people of other colors except white people, it has a 35% error rate when used on non-white women. This is somewhat of a racial and ethical issue that needs to be raised because of the main reason for all these cameras installed is to catch criminal offenders and that means that there is a chance that a different person may be caught for something he/she did not do.\n",
        " \n",
        "Facial recognition techniques are also used in Tesla’s autopilot. When the driver starts autopilot mode, they will take their hands off the wheel, but they still have to focus on road conditions. In order to avoid drivers from falling asleep, the designers of the autopilot mode added a camera in the middle of the car. Facial recognition is used to detect drivers’ facial expressions. When the driver closes their eyes or start yawning, the system will immediately issue a warning. Criminals also find opportunities to ‘get rich’. Criminals can collect enough data and generate the videos from database [GAN] and some private or indecent videos can be created and sent to the owner to extort money.\n",
        " \n",
        "In nowadays society, Facebook use face recognition technology to mark the person’s in the picture, Apple’s brand-new application can identify a single person in videos, Snap’s filters are used with collecting details of each faces. It could be seen that, with the rapid development of internet and internet of things technology, our faces are exposed in various places, while facial information may be used illegally. Imagine of criminals forged facial information so they could log on to someone else’s computer and bank account.Therefore, we have to ensure the privacy of people and accuracy of facial recognition as not to have unethical practices abused in society. Experts should pay more attention on resolving problems like these while they are doing research and designing their systems.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLGXGXvkpytj",
        "colab_type": "text"
      },
      "source": [
        "#References:\n",
        "He, K., Zhang, X., Ren, S. and Sun, J., 2016. ‘Deep residual learning for image recognition’. In Proceedings of the IEEE conference on computer vision and pattern recognition pp. 770-778.\n",
        "\n",
        "Keras, Applications - Keras Documentation, viewed 13 September 2019, <https://keras.io/applications/#resnet>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI-h8fBip86Y",
        "colab_type": "text"
      },
      "source": [
        "#YouTube link for video pitch:\n",
        "https://youtu.be/6PgBzIBrsJc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgaIPqS2rNl1",
        "colab_type": "text"
      },
      "source": [
        "#Link to git:\n",
        "https://github.com/qq1499412503/Data_Assignment2.git"
      ]
    }
  ]
}